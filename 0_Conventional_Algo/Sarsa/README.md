# SARSAアルゴリズムの解説（Python実装付き）

このドキュメントでは、強化学習の一手法である **SARSA（State-Action-Reward-State-Action）** アルゴリズムの概要と、それを用いたPythonプログラムの挙動について解説します。

---

## 📘 SARSAとは？

SARSAは、強化学習における**オンポリシー型の学習アルゴリズム**です。エージェントは現在の方策（ε-greedyなど）に従って行動を選択し、Q値（行動価値）を更新していきます。

Q学習（オフポリシー）と異なり、SARSAは**次に実際に選択した行動のQ値**を使って更新します。

---

## ✅ Q値更新の数式（SARSA）

SARSAでは以下のようにQ値を更新します：

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \cdot Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$

- $Q(s_t, a_t)$：状態 $s_t$ において行動 $a_t$ を取ったときのQ値
- $r_t$：その行動によって得られる即時報酬
- $\gamma$：割引率（将来の報酬の影響度）
- $\alpha$：学習率（Q値の更新の速さ）
- $Q(s_{t+1}, a_{t+1})$：次状態において選択された次の行動のQ値

---

## 🧭 アルゴリズムの流れ

1. Qテーブル（状態×行動の表）を0で初期化
2. 各エピソードで以下を繰り返す：
    - 初期状態 $s_0$ を設定
    - 初期行動 $a_0$ を ε-greedy に基づいて選択
    - エピソード終了まで以下をループ：
        1. 行動 $a_t$ により次の状態 $s_{t+1}$ と報酬 $r_t$ を観測
        2. 次の行動 $a_{t+1}$ を方策に従って選択
        3. Q値を更新
        4. 状態と行動を更新： $s_t \leftarrow s_{t+1}, a_t \leftarrow a_{t+1}$

---

## 🔧 環境の設定（この実装の場合）

- 状態数：6（インデックス0〜5）
- 行動空間：`left`, `right`
- ゴール（`T`）：状態5（終端）
- 報酬設定：
  - ゴールに到達：`+1`
  - 通常移動：`-0.5`
- 表示：エージェントの位置をターミナルに表示
- 学習回数：50エピソード

---

## 📉 学習結果の可視化

各エピソードでのステップ数（ゴールまでにかかった移動回数）を記録し、以下のようにプロットします：

```python
plt.plot(step_counter_times, 'g-')
plt.ylabel("steps")
plt.xlabel("episode")
plt.title("SARSAによる学習の進行")