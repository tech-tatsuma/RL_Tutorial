# Deep Deterministic Policy Gradient (DDPG) アルゴリズム

DDPG は連続アクション空間を持つ環境に適した、アクター・クリティック型のオフポリシー強化学習アルゴリズムです。

### 特徴

* **アクター・クリティック構造**：

  * アクター（Actor）は状態を受け取り、アクションを出力する。
  * クリティック（Critic）は状態とアクションを受け取り、Q値を出力する。
* **オフポリシー学習**：経験リプレイとターゲットネットワークを使用して安定した学習を行う。
* **連続アクション対応**：アクターの出力は連続値であり、環境にそのまま入力される。

---

### 各構成要素

#### 1. アクターネットワーク（Actor）

* 状態を入力とし、連続アクションを出力。
* 出力には `tanh` 関数を使用し、アクションを `[-1, 1]` に正規化し、その後最大値でスケーリング。

#### 2. クリティックネットワーク（Critic）

* 状態とアクションの両方を受け取り、対応する Q 値（価値）を出力。

#### 3. ターゲットネットワーク

* アクターとクリティックのコピー。
* ソフト更新によって学習ネットワークに徐々に追従。
* 学習の安定性向上のために使用。

#### 4. 経験リプレイバッファ

* 過去のトランジション（状態、次状態、アクション、報酬、終了フラグ）を格納。
* ミニバッチでランダムにサンプリングし、相関を減らす。

#### 5. オーンシュタイン・ウーレンベック過程（OU Noise）

* 探索のためのノイズをアクションに加える。
* 時間的に相関のあるノイズを生成し、より物理システム向き。

---

### 学習の流れ

1. 初期化：アクター／クリティックとそのターゲットネットワークを初期化。
2. 環境から状態を観測し、アクションを選択（探索ノイズ付き）。
3. 環境にアクションを適用し、次状態・報酬・終了を観測。
4. トランジションをリプレイバッファに保存。
5. 一定ステップごとにリプレイバッファからサンプリングして以下を実行：

   * ターゲットネットを用いてターゲットQ値を計算。
   * クリティックの損失を `MSELoss` で計算して最適化。
   * アクターの損失を `-Q(s, a)` で定義して最適化。
   * ターゲットネットワークのパラメータをソフト更新。

---

### 注意点

* 離散アクション空間（例：CartPole）には DDPG は適していない。
* 連続アクション空間（例：Pendulum, BipedalWalker）での使用が前提。

## 参考文献

- [Deterministic Policy Gradient Algorithms](https://proceedings.mlr.press/v32/silver14.pdf)
- [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)
- [Deep Deterministic Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#)