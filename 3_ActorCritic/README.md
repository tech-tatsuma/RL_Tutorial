# アルゴリズムの概要：Actor-Critic（REINFORCE with Baseline）

このプログラムは、`CartPole-v1` 環境を対象とした **Actor-Critic** 型の強化学習アルゴリズムを用いた学習実装です。以下に、その手法のポイントを記述します。

## 1. アクター・クリティックとは
Actor-Criticは、以下2つのコンポーネントを持つ強化学習アルゴリズムです：

- **Actor**（方策関数 $\pi_\theta(a|s)$）：状態に対して確率的に行動を出力
- **Critic**（価値関数 $V(s)$）：その状態がどれほど良いか（将来の報酬の期待値）を予測

Actorは、行動の選択確率を学習し、Criticはその選択が良かったかどうかを評価します。

## 2. ネットワーク構成
1つのネットワークから、以下2つの出力を得ます：
- 行動方策：ソフトマックスを通して行動確率を出力（Categorical分布により行動をサンプリング）
- 状態価値：状態に対する価値（スカラー）を出力

## 3. 学習の流れ
各エピソードで以下を行います：

1. エピソード中に状態、行動のlog確率、状態価値、報酬を保存
2. エピソード終了後にリターン（割引報酬）を計算
3. 各ステップで以下を使って損失を計算：
    - ポリシー損失：$-\log\pi(a|s)(R - V(s))$
    - バリュー損失：$(V(s)-R)^2$
4. ポリシー損失とバリュー損失を足し合わせて最小化
