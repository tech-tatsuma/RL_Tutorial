# アルゴリズムの概要：Actor-Critic（REINFORCE with Baseline）

このプログラムは、`CartPole-v1` 環境を対象とした **Actor-Critic** 型の強化学習アルゴリズムを用いた学習実装です。以下に、その手法のポイントを記述します。

## 1. アクター・クリティックとは
Actor-Criticは、以下2つのコンポーネントを持つ強化学習アルゴリズムです：

- **Actor**（方策関数 $\pi_\theta(a|s)$）：状態に対して確率的に行動を出力
- **Critic**（価値関数 $V(s)$）：その状態がどれほど良いか（将来の報酬の期待値）を予測

Actorは、行動の選択確率を学習し、Criticはその選択が良かったかどうかを評価します。

## 2. ネットワーク構成
1つのネットワークから、以下2つの出力を得ます：
- 行動方策：ソフトマックスを通して行動確率を出力（Categorical分布により行動をサンプリング）
- 状態価値：状態に対する価値（スカラー）を出力

## 3. 学習の流れ
各エピソードで以下を行います：

1. エピソード中に状態、行動のlog確率、状態価値、報酬を保存
2. エピソード終了後にリターン（割引報酬）を計算
3. 各ステップで以下を使って損失を計算：
    - ポリシー損失：$-\log \pi(a|s) (R - V(s))$
    - バリュー損失：$(V(s) - R)^2$
4. ポリシー損失とバリュー損失を足し合わせて最小化

## 4. 工夫されている点（DQNからのインスピレーション）
- **Early Stopping**：100エピソードの移動平均報酬が更新されない場合に学習を打ち切る
- **モデル保存**：最良のパフォーマンス時にモデルを保存
- **映像記録**：定期的にプレイ映像を保存し、全体を結合して視覚的に確認可能
- **ログの可視化**：学習報酬曲線をPNGとして出力

## 5. 注意点と改善可能なポイント
- 現在は状態価値のみを基準とするbaselineだが、**GAE（Generalized Advantage Estimation）** の導入でより安定化が期待される
- 方策の更新は毎エピソード後だが、**n-step更新**や**バッチ更新**も検討可能
