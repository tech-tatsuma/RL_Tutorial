# TD3 + Prioritized Experience Replay (PER) 

本プログラムは、連続制御タスク（例: Pendulum）に対して、強化学習アルゴリズム **Twin Delayed Deep Deterministic Policy Gradient (TD3)** を用いてエージェントを訓練し、さらに学習効率を高めるために **Prioritized Experience Replay (PER)** を統合しています。

---

## TD3（Twin Delayed DDPG）とは？

TD3 は、DDPG（Deep Deterministic Policy Gradient）を改善したアルゴリズムで、以下の3つの主要な工夫を加えることで学習の安定性と性能を向上させています：

1. **ツインQネットワーク（Double Q-learning）**:

   * 2つのCritic（Q関数）を持ち、それぞれのQ値を独立に学習。
   * ターゲット値を計算する際に、両者のうち小さい方（min）を使用し、過大評価バイアスを軽減。

2. **ターゲットポリシースムージング**:

   * ターゲットQ値計算時、ノイズを加えたアクションを使うことで過学習を防ぐ。
   * 通常のQ学習よりも堅牢なポリシー更新が可能。

3. **遅延されたポリシー更新（Delayed Policy Update）**:

   * Criticは毎ステップ更新、Actor（ポリシー）は数ステップおきに更新。
   * Criticがある程度安定した後にポリシーを更新することで高品質な勾配が得られる。

---

## Prioritized Experience Replay（PER）とは？

通常の経験再生（Experience Replay）は、過去の経験を均等にサンプリングしますが、PER は「学習に重要なサンプル（TD誤差が大きいもの）」を優先的に再利用します。

* **TD誤差** に基づいて各経験に優先度をつける。
* サンプリング確率は優先度の累乗（$p_i^\alpha$）に比例。
* **重要度サンプリング（Importance Sampling）補正**により、学習の偏りを防ぎ、バイアスを軽減する。

---

## 状態の正規化

環境から得られる状態のスケールが大きく異なる場合、学習が不安定になることがあります。
このプログラムでは `RunningMeanStd` により状態を正規化することで、
学習中の安定性を向上させています。

---

## 主な構成要素

* **Actor**: 状態を入力とし、連続アクションを出力。
* **Critic1, Critic2**: 状態とアクションを入力とし、Q値（価値）を出力。
* **リプレイバッファ**: PER を用いたバッファ。TD誤差で優先度を更新。
* **状態正規化**: `RunningMeanStd` により、状態の平均・分散を逐次更新し、正規化。

---

## 実行フロー

1. 環境から初期状態を取得。
2. Actorからアクションを取得（探索ノイズ追加）。
3. 環境にアクションを与え、次の状態・報酬を観測。
4. 経験（s, s', a, r, done）をリプレイバッファに格納。
5. 毎ステップ、サンプルをバッチで取り出し、Criticを更新。
6. 指定ステップ毎に Actor を更新し、ターゲットネットワークをソフト更新。
7. 一定間隔で動画や学習曲線を保存。