# 0_Conventional_algoの概要説明
## 実装アルゴリズム
- Q-Learning
- SARSA

## 想定環境
### Simple

この環境は、0〜Nまでの連続した整数による**1次元の状態空間**で構成されます。各状態は、単純に左か右に移動できるだけのマスで、右端の状態がゴールとして定義されています。

- 状態は `[0, 1, 2, ..., N-1]` のように並んでおり、`N-1` の次がゴール（`terminal`）になります。
- エージェントは常に状態0からスタートし、"left" または "right" の2つの行動を選択できます。
- "right" を選んで `N-2` の位置にいると、次のステップで "terminal" に到達します。
- 通常の移動には報酬 `-0.5` が与えられ、ゴールに到達したときのみ `+1` の報酬が得られます。
- 環境の状態は文字列として `['-', '-', ..., 'T']` の形で表示され、エージェントの位置には `'*'` が表示されます。

この環境は非常にシンプルで、アルゴリズムの基本的な動作（状態遷移・報酬獲得・Q値更新）を視覚的に確認するために適しています。行動選択にはε-greedy法が用いられており、学習が進むにつれて最短経路を効率的に見つけられるようになります。
### GridWorld
GridWorldは、強化学習における典型的な2次元環境であり、より現実に近い状態空間を扱う学習が可能です。エージェントは上下左右に移動でき、障害物やゴール、異なる報酬を持つマスを自由に定義することができます。

- 環境は `row × col` のグリッドで構成され、各セルは状態・報酬・マスの種別を持っています。
- マスの種別は以下の通りです：
  - `0`: 通常マス（通行可能・報酬なし）
  - `-1`: 障害物（通行不可）
  - `+1`: 終端状態（ゴール）
- エージェントは初期状態として左下（最下行・最左列）から開始し、終了状態に到達するとエピソードが終了します。
- `transition_matrix` により、行動にノイズを持たせることが可能です。例えば、"UP" を選んでも 80% の確率で上に進み、10% の確率で右や左にずれるなど、現実的な移動の不確実性を再現できます。
- 各マスに設定された報酬は `reward_matrix` で管理され、ゴールには +1、通常マスは 0、または自由に設定可能です。
- 行動は整数で表され、以下のように対応しています：
  - `0`: UP
  - `1`: RIGHT
  - `2`: DOWN
  - `3`: LEFT
- エージェントの現在位置は `render()` メソッドによりターミナルに可視化され、次のような表記になります：
  - `○`: エージェントの位置
  - `-`: 通常のマス
  - `#`: 障害物
  - `*`: 終端状態（ゴール）

この環境は、より複雑な戦略学習や経路探索タスクの評価、SARSAやQ-learningなどのアルゴリズムの挙動の違いを確認するために非常に有用です。エージェントは障害物を避け、報酬を最大化するルートを学習していきます。

## 実行方法
`0_Conventional_Algo/`から以下のようなコマンドを実行
```
python -m Q-Learning.simple
```