# PPO（Proximal Policy Optimization）

このドキュメントは、`Pendulum-v1` 環境において **PPO（Proximal Policy Optimization）** アルゴリズムを用いてエージェントを訓練するプログラムの概要を説明します。

## PPOとは

PPOは、方策勾配法の改良版であり、旧方策と新方策の間の変化をクリッピングによって制限することで、安定した学習を実現します。大きな変更による崩壊を防ぎつつ、十分な改善も可能にします。

## 使用されている主な構成要素

### 1. Actor ネットワーク

* 状態からアクションの平均（mu）と標準偏差（sigma）を出力します。
* 出力された正規分布からアクションをサンプリングし、その対数確率（$`log_prob`$）を記録します。

### 2. Critic ネットワーク

* 状態からその価値（$`V(s)`$）を予測します。
* TD誤差ではなく、Advantage（$`A=R-V(s)`$）を使って更新されます。

### 3. クリッピング手法

* 方策比率 `r = π(a|s)/π_old(a|s)` を使って、
  `L = min(r * A, clip(r, 1-ε, 1+ε) * A)` を最大化。
* これにより過度な更新が制限され、安定した学習が可能になります。

### 4. リプレイバッファとミニバッチ

* 経験を一時的にバッファに蓄積し、バッファサイズに達したら学習。
* 複数エポックに分けてミニバッチで更新します（PPO-Clip 方式）。